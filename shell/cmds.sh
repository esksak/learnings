# test：処理結果を終了ステータスとして返す。
# test expression / [ expression ] ([ ]の場合はスペースを開ける必要がある)
# 単項式：-hoge と処理対象からなり、多くの場合ファイル名が指定される
# 二項式：２つの処理対象と比較のための演算子からなる。引数がひとつの場合はその値がnullでないか判断される
# [ ! -x "$file" ]のような形式で使う。""がないと引数がnullだった場合に引き渡されないため、必須
# -a/-oはそれぞれand/orだが、各条件が全て評価されるので非推奨。&&と||を使用する。
# 正常終了が0なので、trueが0、それ以外がfalse

# testコマンドでの表現
# 演算子　 ｜　trueが返される条件
# -------------------------------
# string   ｜　stringがnullでない
# -n string｜  stringがnullでない
# -z string｜　stringがnullである
# -d file  ｜　fileがディレクトリの場合
# -e file  ｜　fileが存在する場合
# s1 = s2　｜　文字列s1とs2が等しい
# s1 != s2 ｜　文字列s1とs2が等しくない
# n1 -eq n2｜　整数n1とn2が等しい
# n1 -ne n2｜　整数n1とn2が等しくない
# n1 -lt n2｜　整数n1がn2未満
# n1 -gt n2｜　整数n1がn2より大きい
# n1 -le n2｜　整数n1がn2以下である
# n1 -ge n2｜　整数n1がn2以上である

# -b file  ｜  fileがブロックデバイスファイルの場合
# -c file  ｜　fileがキャラクタデバイスファイルの場合
# -f file  ｜  fileが通常のファイルである
# -g file  ｜　fileにsetgidビットがセットされている
# -h file  ｜　fileがシンボリックリンクである
# -L file  ｜　fileがシンボリックリンクである(-hと同様)
# -p file  ｜　fileが名前付きパイプ（FIFOファイル）である
# -r file  ｜　fileが読み取り可能
# -S file　｜　fileがソケットである
# -s file　｜　fileが空でない
# -t n     ｜　file記述子nが端末を指している場合
# -u file  ｜　fileにsetuidビットがセットされている場合
# -w file  ｜　fileが書き込み可能な場合
# -x file  ｜　fileが実行可能なファイルか、中のファイルを検索可能なディレクトリの場合

# tr: 文字列の置換（translate）。sedの簡易版
# 大文字を小文字にしたり、特定の文字や連続している文字を削除するためのオプションも用意されている
# tr [ options ] source replace
# -c sourceの意味を反転し、sourceに無いものを見るようにする
# -C -cとほぼ同義で、マルチバイト文字に対応
# -d sourceをreplaceから削除する
# -s 連続した文字を切り詰める。連続でsourceの文字がある場合は一つにする
tr -d '¥r' < file_win > file_unix
echo "hellllllllo" | tr -s \[a-z\]  # => helo

# cut：特定の文字列の切り出し。awkの簡易版
# 入力されたデータから特定のフィールドあるいは特定の範囲の文字列を取り出す
# cut -c list [ file ]
# cut -f list [ -d delim ] [ file ]
# -c list 文字単位で切り出し。listには例えば1,3,5-12,42のように文字の位置または範囲をカンマで区切って指定
# -d delim delimを区切り文字として使う。-f併用。指定がない場合はタブが選択される
# -f list フィールド単位で切り出す。listにはフィールドの番号またはその範囲をカンマで区切って指定。フィールドのdelimiterは-dで指定

# 全ユーザーのホームディレクトリを表示（delimiterを:、6つ目のフィールドを切り出し）
cut -d : -f 6 /etc/passwd

# 文字列の中から特定の箇所を切り出し
echo "abcdefghijklmnopqrstuvwxyz" | cut -c 1,3,9-20 

# sed：入力として受け取ったストリームのデータを編集し、結果を標準出力に書きだす。名前の由来はStream Editorから。
# sed [ -n ] 'editing command' [ files ]
# sed [ -n ] -e 'editing command' - [ files ]
# sed [ -n ] -f script-file - [ files ]
# -n 修正された行の出力を行わない。
# -e 'editing commnad' 入力のデータに対してediting commnad を実行。
# -f script-file コマンドをファイルから読み込む。実行コマンドがたくさんある場合便利
# 正規表現はデフォルトで BRE、-E か -r オプションで ERE
# マッチングは最大のところをとってしまうので注意

# editing commandは前半と後半に分かれる。
# 前半は、処理する行を指定でき、後半で処理を指定できる

# 前半のパターン
# n: n 行目、$ は最後の行を表す
# n,m: n行目からm行目
# n~m: n行目からm行ごと
# n,~m: n行目から次のmの倍数行まで
# /regexp/: 正規表現 regexp とマッチする行
# /regexp/!: マッチしない行
# /re1/,/re2/: re1にマッチした行からre2 にマッチした行まで
# デリミターは/が使えるが、それ以外を使う場合はエスケープする必要があるっぽい？

# 後半のパターン
# = 行番号を前の行に表示(-n オプションで行番号だけを抽出)
# a word 文字列 word をマッチした行の次の行に追加
# i word 文字列 word をマッチした行の前の行に追加
# q 処理をそこで終了する
# c word 選択された行を指定された文字列で置換（word には\nを含んでよい)
# d	行を削除
# p	行を表示 (-nオプションで条件に合う行だけ抽出)
# s/regexp/replace/	正規表現 regexp とマッチする文字列を replace に置換。
# デリミターは何でも良く（\など、一部使えないものもある）;やスペースだと読みやすいかも
# y/target/replace/	target の各文字を replace の各文字に変換(両者は同じ長さでなければならない)

# 全部置換
sed 's;/home/tolstoy/;/home/lt/;g'

# 二番目にヒットしたところだけ置換できる
sed 's;/home/tolstoy/;/home/lt/;2'

# <HTML>に一致するところの行だけ出力する
sed -n '/<HTML>/p' *.html

# マッチングの対象となる行を絞り込んで置換
# s内の$は最終行（ただし、複数ファイルの場合は最後のファイルの最終行と判断される）を意味する
# oldfuncに該当した行のうちの最終行を、# commentで置換する、という意味？
sed -e '/oldfunc/ s/$/# comment/' filename

# s内の//（空）は、マッチングに利用した正規表現（この場合/Tolstoy/）を意味する
# &は、マッチングに利用した正規表現でマッチした文字列を意味するっぽい？
sed -e '/Tolstoy/ s//& and Camus/g' filename

# 否定の正規表現（!の後に空白を入れると未定義になるので、スペースを入れない）
sed '/used/!s/new/used/g' filename

# awk：フィールドの抽出や並び替えに便利なように設計されたプログラミング言語
# awk 'program' [ file ]
# コマンドの形や指定されたファイルから１行ずつ読み出すところなどがsedと共通している

# 基本構文
# pattern { action }
# pattern { action }
# pattern { action }
# ....
# patternとactionは、どちらか一方だけでも良い。
# patternはEREで記述され、actionはprintが使われることが多い
# 初期化と終了処理はそれぞれBEGIN { startup code} , END { clean up code }で定義できる
# gsub("x", "y", $x)で$x内のxをyに全て置換できる。$xに$0を当てはめると、各行全体に対して実行できるので便利

# FS: 入力の区切り文字。正規表現も入れられる。デフォルトは空白  -Fでも指定できる
# OFS: 出力の区切り文字
# ${数字}: 各フィールドを取り出せる。ただし$0はレコード全体という意味がある。
# NF:フィールドの数。Number of Fieldsの略？

awk -F: 'OFS=" -- " { print $1, $2, $3, $4, $5}' /etc/passwd

# sort：各行の並び替え
# 指定されたフィールドの値に基づき、データの型や使われているロケールを考慮して各行を並べ替える
# sort [ options ] [ files ]
# -b 先頭の空白を無視
# -c 入力されたデータが既に正しく並べ替えられているかをチェック。出力は行わず、終了ステータスだけ返す
# -d 辞書順に並べ替え
# -g フィールドの値を浮動小数点として比較
# -f 大文字と小文字を区別しない
# -i 印字不能文字を無視
# -k 並べ替えに利用するキーとなるフィールドを指定
# -m 複数ファイルに含まれているデータをまとめてから並べ替える。各ファイルは既に並べ替えられている必要がある
# -n フィールドの値を整数値として比較
# -o outfile ファイルの出力先を指定
# -r 降順に並べ替え
# -t char char1文字をフィールドの区切り文字として仕様。
# -u 重複キーを除く。キーが同じであればフィールドが異なっていても出力しない

# uniq：重複レコードを削除して出力。sortからパイプでつないで活用される。
# uniq [ file ]
# -c レコードの出現回数も表示
# -d 重複している行だけ出力
# -u 重複していない行だけ出力

history -500 | cut -d ' ' -f 3 | sort | uniq -c | sort -nr 

# read: 読み込んだ行をIFSに基づいて分割し、１つ以上の変数にセットする
# read [ -r ] cariable...
# -r: 文字通りに読み込む。行末にバックスラッシュが入力されていてもデータが次の行にも続くという意味には解釈されない

# 標準入力からデータを1行ずつ読み込み、区切り文字（変数$IFSで指定）を使ってフィールド単位に分解する
# フィールドが指定された変数の数より多かった場合、残りのフィールドの値を含めて最後の変数にセットされる
# 行末にバックスラッシュが含まれている場合、このバックスラッシュと直後の改行文字は無視され、
# 次の行に対する読み込みが行われる。-rがあるとバックスラッシュが通常文字として扱われる

while IFS=: read user pass uid gid fullname homedir shell
do
    ...
done < /etc/passwd

# exec: 現在実行されているシェルを新しいプログラムで置き換えたり、
# シェルの入出力に関する設定を変更したりする
# exec [ program [ arguments ... ] ]

exec 2> /tmp/$0.log #標準エラー出力を変更する
exec 3< /some/file #3という記述子を新たに割り当てる

# プロンプトは標準エラー出力なので、プロンプトが表示されなくなるので、先にキャッシュして戻せるようにしておく
exec 5>&2 # 5に2の出力先をキャッシュ
exec 2> /tmp/$0.log
...
exec 2>&5 #2をもとに戻す
exec 5>&- #5をclose

# 引数つきで実行すると、現在のシェルの代わりに指定されたプログラムが実行される
# つまり同じプロセスとして新しいプログラムが起動される。
# 例えば、シェルスクリプトを使ってコマンドライン引数を処理し、
# 以降の処理は全て別のプログラムにまかせてしまうという場合に使える。

# dd: データを指定された大きさのブロック単位で指定された数コピーする
# od：バイト列をASCII, 8進、16進のそれぞれで表示する
# strings：データの中から4文字以上かつ改行かヌル文字で終わる印刷可能文字を探して標準出力に書き出す。
# コンパイルされたデータなどのバイナリを調べたいときに有効

# export/readonly：exportは環境の値を変更・表示する。readonlyは変数の値を変更できなくする。
# export/readonly name
# -p: 環境にセットされている値/読み取り専用の値を全て出力する。同じ環境にするためのスクリプト形式。
export -p
readonly -p

# unset：現在実行されているシェルで変数・関数を削除する
# -f 指定された関数を削除する
# -v 指定された変数を削除する。変数だと明示したいときにつけるっぽい。
unset hoge

# env：環境の内容をより詳細に指定してコマンドを実行できる
# env [ -i ] [ var=value ... ] [command_name [ arguments ... ] ]
# -i 現在の環境の内容を全て無視し、コマンドラインで指定された変数のみを使用する
env -i RAILS_ENV=staging bundle exec rails s

# getopts：引数の処理の簡略化とPOSIXで定義された引数の記述方法に対応する
# getopts option_spec variable [ arguments ... ]
# 最初の引数では処理できるオプションの文字を列挙する。オプションに引数が必要な場合は、その文字に続けてコロンを記述する
# 対応する引数の値は、変数OPTARGにセットされる。
# 不正なオプションの場合は？がセットされる
# ちなみに、getoptがある（GNU版とオリジナルの２つ）が、移植性が高いのはgetopts。
# GNU版のgetoptは高機能だが、Macなどには標準で入っていない

# 初期化
file= verbose= quiet= long=

#先頭にコロンをつけると自分でエラー処理を行える。付けないと'?'は多分無視される
while getopts :f:vql opt
do
    case $opt in　#オプションの文字をチェック
    f)  file=$OPTARG　#optにはマイナス記号はsetされない
        ;;
    v)  verbose=true
        quiet=
        ;;
    q)  quiet=true
        verbose=
        ;;
    l)  long=true
        ;;
    '?') echo "$0 不正なオプションです -$OPTARG" >&2
       #不正な文字の場合は、OPTARGにその文字がセットされる
         exit1
        ;;
    esac
done
#オプションを全て消して、コマンドラインからの引数だけを残す
shift $((OPITIND -1 )) 

# wc：文字カウント
# -l で行数カウント
# -c バイト数カウント（マルチバイト文字がなければ＝文字数カウント）
# -w 単語数カウント
# -m 文字数カウント（マルチバイト文字がある場合もＯＫ）
wc -l filename.log

# join：共通のキーに基づいてレコードを結合する。
# join [ options ] file1 file2
# -1 field1 , -2 field2 結合のキーとなるフィールドを指定。
# field1はfile1中のfield1番目、field2はfile2のfield2番目のフィールドを意味する。
# -o file.field file番目（1or2）のファイルのfield番目のフィールドを出力
# -t separator 空白の代わりにseparatorを区切り文字として使用する。

# fmt：段落の整形。一行あたり何文字まで出力するとかを制限できる。
# 簡単なものなら問題ないが、複雑なことをしようとすると、処理系依存になる。
# -w width 出力される行の長さをwidthに制限
# -s 長すぎる行のみを分割

# file：引数として指定されたファイルのそれぞれについて先頭の数バイトを読み込み、形式を推定して表示する
